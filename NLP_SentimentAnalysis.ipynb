{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "### Loading the dataset (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\punit\\Anaconda3\\envs\\aiml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split ( 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZhMAgaNeCy5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    1,   14,   22,   16,   43,  530,\n",
       "        973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,   36,\n",
       "        256,    5,   25,  100,   43,  838,  112,   50,  670,    2,    9,\n",
       "         35,  480,  284,    5,  150,    4,  172,  112,  167,    2,  336,\n",
       "        385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,\n",
       "          4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,    4,\n",
       "       1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,\n",
       "         38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,\n",
       "         16,  626,   18,    2,    5,   62,  386,   12,    8,  316,    8,\n",
       "        106,    5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,\n",
       "        130,   12,   16,   38,  619,    5,   25,  124,   51,   36,  135,\n",
       "         48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,\n",
       "          5,   14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,\n",
       "         15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model (30 points)\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras import regularizers, optimizers, losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               2400250   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,720,501\n",
      "Trainable params: 2,720,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Init_model = Sequential()\n",
    "Init_model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
    "Init_model.add(Flatten())\n",
    "Init_model.add(Dropout(0.5))\n",
    "Init_model.add(Dense(250, activation='relu'))\n",
    "Init_model.add(Dropout(0.5))\n",
    "Init_model.add(Dense(1, activation='sigmoid'))\n",
    "Init_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(Init_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                153616    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 473,633\n",
      "Trainable params: 473,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimized_model = Sequential()\n",
    "optimized_model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
    "optimized_model.add(Flatten())\n",
    "optimized_model.add(Dropout(0.5))\n",
    "optimized_model.add(Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\n",
    "optimized_model.add(Dropout(0.5))\n",
    "optimized_model.add(Dense(1, activation='sigmoid'))\n",
    "optimized_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(optimized_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumEpochs = 8\n",
    "BatchSize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 15s 604us/step - loss: 0.5261 - acc: 0.7067 - val_loss: 0.3182 - val_acc: 0.8631\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 14s 577us/step - loss: 0.2261 - acc: 0.9102 - val_loss: 0.3040 - val_acc: 0.8710\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 14s 573us/step - loss: 0.1239 - acc: 0.9535 - val_loss: 0.3365 - val_acc: 0.8714\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 14s 573us/step - loss: 0.0748 - acc: 0.9733 - val_loss: 0.3865 - val_acc: 0.8704\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 14s 575us/step - loss: 0.0538 - acc: 0.9800 - val_loss: 0.4390 - val_acc: 0.8681\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 15s 583us/step - loss: 0.0400 - acc: 0.9852 - val_loss: 0.4667 - val_acc: 0.8708\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 14s 576us/step - loss: 0.0352 - acc: 0.9869 - val_loss: 0.4966 - val_acc: 0.8718\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 14s 573us/step - loss: 0.0302 - acc: 0.9901 - val_loss: 0.5087 - val_acc: 0.8732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef4e3f0ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Init_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=NumEpochs, batch_size=BatchSize, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 6s 258us/step - loss: 0.8267 - acc: 0.5008 - val_loss: 0.7809 - val_acc: 0.5154\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 6s 249us/step - loss: 0.7757 - acc: 0.6091 - val_loss: 0.7816 - val_acc: 0.7382\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 6s 249us/step - loss: 0.7399 - acc: 0.7554 - val_loss: 0.7055 - val_acc: 0.8336\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 6s 248us/step - loss: 0.6883 - acc: 0.8094 - val_loss: 0.6599 - val_acc: 0.8523\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 6s 254us/step - loss: 0.6535 - acc: 0.8419 - val_loss: 0.6268 - val_acc: 0.8641\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 6s 254us/step - loss: 0.6322 - acc: 0.8618 - val_loss: 0.6309 - val_acc: 0.8691\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 6s 246us/step - loss: 0.6171 - acc: 0.8734 - val_loss: 0.6134 - val_acc: 0.8755\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 6s 248us/step - loss: 0.6052 - acc: 0.8810 - val_loss: 0.6449 - val_acc: 0.8737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef4ed7ef60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model.fit(x_train, y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Accuracy of the model  & Retrive the output of each layer in keras for a given single test sample from the trained model you built (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AqOnLa2eCzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 113us/step\n",
      "Accuracy: 87.32%\n"
     ]
    }
   ],
   "source": [
    "scores_init_model = Init_model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_init_model[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dUDSg7VeCzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 65us/step\n",
      "Accuracy: 87.37%\n"
     ]
    }
   ],
   "source": [
    "scores_optimized_model = optimized_model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_optimized_model[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> We could see init model is overfitting, train accuracy is huge but validation accuracy is pretty low.<br>\n",
    "optimized model has reduced overfitting when we added dropout and regularization and accuracy also increased</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tskt_1npeCzP"
   },
   "source": [
    "### Retrive the output of each layer in keras for a given single test sample from the trained model you built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "\n",
    "inp = optimized_model.input\n",
    "outputs = [layer.output for layer in optimized_model.layers]\n",
    "functions = [k.function([inp, k.learning_phase()],[out]) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00694243  0.00213523  0.0001971  -0.00136693 -0.00631926 -0.00097206\n",
      "  0.00363801  0.00121643 -0.0012039   0.00234943 -0.0079347   0.00384465\n",
      " -0.00143604  0.00505743 -0.00050236 -0.0036982  -0.0013103   0.00368813\n",
      " -0.00563823  0.00676969  0.00074808 -0.01156617  0.0130675  -0.00483128\n",
      " -0.0031705  -0.00050625 -0.00406987  0.00439637  0.00095135 -0.0079884\n",
      "  0.00017574  0.00020696]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([x_test[11],])\n",
    "layer_outs = [func([test,1.]) for func in functions]\n",
    "print(layer_outs[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
